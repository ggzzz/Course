{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3. 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    c = jnp.max(x)\n",
    "    exp_x = jnp.exp(a-c)\n",
    "    sum_exp_x = jnp.sum(exp_x)\n",
    "    y = exp_x / sum_exp_x\n",
    "\n",
    "    return y\n",
    "\n",
    "def softmax_batch(x):\n",
    "    x = x - jnp.max(x, axis=-1, keepdims=True)   # オーバーフロー対策\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "# test\n",
    "x = jnp.array([-1.0, 1.0, 2.0])\n",
    "print(jnp.allclose(sigmoid(x), jnp.array([ 0.26894142, 0.73105858, 0.88079708])))\n",
    "\n",
    "a = jnp.array([0.3, 2.9, 4.0])\n",
    "print(jnp.allclose(jnp.sum(softmax(x)), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "# 第一次调用会花费几分钟......\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "# 输出各个数据的形状 \n",
    "print(x_train.shape) # (60000, 784)\n",
    "print(t_train.shape) # (60000,) \n",
    "print(x_test.shape) # (10000, 784) \n",
    "print(t_test.shape) # (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(784,)\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APAACzBVBJJwAO9dnp/wm8damu6Dw5dRjGf9IKw/+hkVPffCnWNJa7XVNV0Kxa1hErrNe/M2cnYqgElsAHpjkc1wlAODkV694W8c654t8M6n4TuvEctrrFw0cun3c0/lq+3AMJcDK5AyOeTkd+fPvGFn4gsvEtzF4m89tUG1ZJJjuMgUBVYN/EMKOe9YVXtK0bUtdvVs9LsZ7y4YgbIULYycZPoPc8V6lpfwh0/w7p66z8RdXj0y2z8llC4aWQ+mRn8lz9RXPfE3x1pvi46TYaPZTQadpMJghluWDSyrhQM9SMBe5Oc5NcBV7Tda1XRZJJNK1O8sXkG12tZ2iLD0JUjNQ3l9eahN517dT3MvTfNIXb16n6mq9Ff/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4AWNgGMyAWUhIqK5jvdSy/9/rQe5kgTlWjs3KRiAYxHsyKfDzxYMgFiOIAALDvfwQBsO/pK8Mz97fhPLAlNDtvyBwbNv3j8jCUHbAnOy/f89yM2jPwiLJwMc4628UqgQTnPvp/0eGFAQXLg5lcO/764YuhuArf3y4IAfmfoQwlBX44e/fckkMYaiA7q6/f6dJ45IViP3zdzcuSQaGn39/OkBl4WEL4euFmLIwXDuETav6lKfAIPy1DYucRNFdUPCe9MOUE3e6CpI6FogZSEKrwbFyOIATQ5v5mkcgXV9auVGlwK4NDGRguL75b88HVDla8QBFF16ADQA8sQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist \n",
    "from PIL import Image\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img)) \n",
    "    display(pil_img)\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "img = x_train[0]\n",
    "label = t_train[0]\n",
    "print(label) # 5\n",
    "print(img.shape) # (784,)\n",
    "img = img.reshape(28, 28) # 把图像的形状变成原来的尺寸 \n",
    "print(img.shape) # (28, 28)\n",
    "img_show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = \\\n",
    "        load_mnist(normalize=True, flatten=True, one_hot_label=False) \n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"sample_weight.pkl\", 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3'] \n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    a1 = jnp.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = jnp.dot(z1, W2) + b2 \n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = jnp.dot(z2, W3) + b3\n",
    "    y = softmax_batch(a3)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x, t = get_data() \n",
    "network = init_network()\n",
    "batch_size = 100 # 批数量\n",
    "accuracy_cnt = 0\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis=1)\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4. 神经网络的学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t): \n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 初始化权重 \n",
    "        self.params = {} \n",
    "        self.params['W1'] = weight_init_std * jax.random.normal(key, shape=(input_size, hidden_size))\n",
    "        self.params['b1'] = jnp.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * jax.random.normal(key, shape=(hidden_size, output_size))\n",
    "        self.params['b2'] = jnp.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        a1 = jnp.dot(x, W1) + b1 \n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = jnp.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = jnp.argmax(y, axis=1) \n",
    "        t = jnp.argmax(t, axis=1)\n",
    "        accuracy = jnp.sum(y == t) / float(x.shape[0]) \n",
    "        return accuracy\n",
    "    \n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1']) \n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1']) \n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2']) \n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operand array with iterator write flag set is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m t_batch \u001b[39m=\u001b[39m t_train[batch_mask]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# 计算梯度\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m grad \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39;49mnumerical_gradient(x_batch, t_batch) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# grad = network.gradient(x_batch, t_batch) # 高速版!\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# 更新参数\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb2\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;32m/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m loss_W \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, t)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb#X13sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m grads \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m grads[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m numerical_gradient(loss_W, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mW1\u001b[39;49m\u001b[39m'\u001b[39;49m]) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m grads[\u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m numerical_gradient(loss_W, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m]) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/m/Documents/GitProjects/Course/ML/DL/DL_from_scratch/DL_from_scratch.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m grads[\u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m numerical_gradient(loss_W, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m]) \n",
      "File \u001b[0;32m~/Documents/GitProjects/Course/ML/DL/DL_from_scratch/common/gradient.py:38\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m h \u001b[39m=\u001b[39m \u001b[39m1e-4\u001b[39m \u001b[39m# 0.0001\u001b[39;00m\n\u001b[1;32m     36\u001b[0m grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(x)\n\u001b[0;32m---> 38\u001b[0m it \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mnditer(x, flags\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mmulti_index\u001b[39;49m\u001b[39m'\u001b[39;49m], op_flags\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mreadwrite\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     39\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m it\u001b[39m.\u001b[39mfinished:\n\u001b[1;32m     40\u001b[0m     idx \u001b[39m=\u001b[39m it\u001b[39m.\u001b[39mmulti_index\n",
      "\u001b[0;31mValueError\u001b[0m: operand array with iterator write flag set is read-only"
     ]
    }
   ],
   "source": [
    "# mini-batch\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "train_loss_list = []\n",
    "\n",
    "# 超参数\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0] \n",
    "batch_size = 100 \n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num): \n",
    "    # 获取mini-batch\n",
    "    # batch_mask = np.random.choice(train_size, batch_size) \n",
    "    batch_mask = jax.random.choice(key, train_size, shape=(batch_size,), replace=False)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 计算梯度\n",
    "    grad = network.numerical_gradient(x_batch, t_batch) \n",
    "    # grad = network.gradient(x_batch, t_batch) # 高速版!\n",
    "    \n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 记录学习过程\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
